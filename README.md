# Sentiment Analysis with Natural Language Processing and Machine Learning

### Introduction
Language is one of the fundamental pillars of our society so it comes as no surprise that it can be an incredibly powerful form of data if it is processed and analyzed effectively. The ability to analyse every book in a library or every political article from the past decade is something that is only efficiently possible using computer science which opens up many powerful avenues to augment a decision making process. From product reviews to social media posts to novels and articles the mediums in which our language can be conveyed is diverse and complex. This diversity and complexity, along with the inherent nuances innately found in how we communicate, requires special techniques and strategies to be applied before it can be used to train machine learning models and make meaningful predictions. This project will explore those strategies and apply them to various types of machine learning models.

A common use for natural language processing (NLP) is to determine the overall sentiment regarding a specific subject. This subject could be anything from a product review to someone's tweet history. This project will outline those processing steps, test and compare various optimized machine learning models, and build a production ready tool that predicts the positive and negative sentiment of product reviews and output lists with those predictions. The focus of this project will be to achieve an accuracy and recall of 80% or higher.

### Model Results
Based on the results of this it is very obvious that our model is not capable of correctly determining which reviews are negative evident by the recall scores for the three classes being 72% positive, 25% negative, and 89% neutral. In fact a negative prediction was the least likely outcome for a negative review to be classified as. Based on the data available we are not able to train a model confidently to classify these 3 classes. This is likely due to the "neutral" class being a weird grey area between positive and negative and deteriorating the boundaries between the two.

To work around this issue we'll try two potential solutions. The first will be to agglomerate the negative and neutral data and turn our dataset into a binary classification containing 'positive' reviews and 'non-positive' reviews. Since our original dataset was a 50/30/20 spilt in the first place this will also help handle any potential class imbalance issues. The second method will be to drop the neutral reviews altogether. This will exasperate our class imbalance so we will pair this with SMOTE oversampling.

After removing the neutral features and oversampling the negative features to build a balanced data set we have achieved the best scores on all of our metrics throughout this project. Recall scores of 90% and 85% for the positive and negative features respectively. Based on the steps that we have taken I am confident that we have found the optimal model and set up to classify reviews with positive and negative sentiments.

### Test Results and Conclusions
Based on the true values of the test data set we can see that roughly 25% of the positive data is being classified as negative. This is not within the arbitrary goals of the project and does not foster confidence in the model to effectively categorize this data. I believe that this project was thorough in its model selection and optimization as well as exploring different data manipulations to get the best results with this dataset and that the issues may lie in the dataset itself. 

This project trained it's models using only ~1650 tokens which is likely too little to be able to decode portions of an entire language. The number of tokens could easily be increased by reducing the minimum document frequency within the tokenizer parameters that we used. This issue with simply doing that to increase the number of tokens the model is trained on is that it would lead to more over-fitting and not necessarily improve it's performance when predicting on new data. More complicated models, like neural networks, could also be applied but they would also suffer from the size and breadth of our current dataset. In summary we believe that to build an Natural Language Processing model for the purposes of sentiment analysis is it important that your training dataset be very large. This would ensure that your model is training off of a diverse yet specific vocabulary and have the ability to effectively predict text that it was not trained on.

### Test Results and Conclusions
Based on the true values of the test data set we can see that roughly 25% of the positive data is being classified as negative. This is not within the arbitrary goals of the project and does not foster confidence in the model to effectively categorize this data. I believe that this project was thorough in its model selection and optimization as well as exploring different data manipulations to get the best results with this dataset and that the issues may lie in the dataset itself. 

This project trained it's models using only ~1650 tokens which is likely too little to be able to decode portions of an entire language. The number of tokens could easily be increased by reducing the minimum document frequency within the tokenizer parameters that we used. This issue with simply doing that to increase the number of tokens the model is trained on is that it would lead to more over-fitting and not necessarily improve it's performance when predicting on new data. More complicated models, like neural networks, could also be applied but they would also suffer from the size and breadth of our current dataset. In summary we believe that to build an Natural Language Processing model for the purposes of sentiment analysis is it important that your training dataset be very large. This would ensure that your model is training off of a diverse yet specific vocabulary and have the ability to effectively predict text that it was not trained on.
